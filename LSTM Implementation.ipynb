{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2361c271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "import pickle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "import datetime as dt\n",
    "import random\n",
    "import os \n",
    "import h5py\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "PATIENCE = 20\n",
    "MAX_EPOCH = 200 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7ba0efc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fbdc0445",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.load('tes_data_sli.npy')\n",
    "test_lab = np.load('tes_lab.npy')\n",
    "\n",
    "train_data = np.load('tra_data_sli.npy')\n",
    "train_lab = np.load('tra_lab.npy')\n",
    "\n",
    "val_data = np.load('val_data_sli.npy')\n",
    "val_lab = np.load('val_lab.npy')\n",
    "\n",
    "\n",
    "\n",
    "input_size = train_data.shape[1]\n",
    "output_size = train_lab.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cffefb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples are 725\n",
      "Number of validation samples are 204\n",
      "Number of test samples are 100\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training samples are {}\".format(train_data.shape[0]))\n",
    "print(\"Number of validation samples are {}\".format(val_data.shape[0]))\n",
    "print(\"Number of test samples are {}\".format(test_data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9716cf55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(725, 280, 1)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741b6a00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "881dce50",
   "metadata": {},
   "source": [
    "# LSTM Model Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d0056f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_16 (LSTM)              (None, 280, 32)           4352      \n",
      "                                                                 \n",
      " lstm_17 (LSTM)              (None, 32)                8320      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 56)                1848      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,520\n",
      "Trainable params: 14,520\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(32,activation='sigmoid',return_sequences=True,input_shape=(train_data.shape[1],train_data.shape[2])),\n",
    "    tf.keras.layers.LSTM(32,activation='sigmoid',return_sequences=False),\n",
    "    tf.keras.layers.Dense(units=56)\n",
    "]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "507e68b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(\n",
    "    lambda epoch : ((1e-4)*(10**(epoch/40))) if (1e-4)*(10**(epoch/20)) < 1e-2 else 10**-2\n",
    ")\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "\n",
    "ear_stop = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\",mode='min',min_delta=5e-4,patience=20,verbose=1\n",
    ")\n",
    "\n",
    "model_chk = tf.keras.callbacks.ModelCheckpoint(\n",
    "            'best_model.h5',monitor=\"val_loss\",mode='min',verbose=1,save_best_only=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "66445d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=adam,loss=tf.keras.losses.Huber(),metrics=[\"mean_squared_error\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f7fd6ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3825 - mean_squared_error: 0.8599\n",
      "Epoch 1: val_loss improved from inf to 0.38015, saving model to best_model.h5\n",
      "23/23 [==============================] - 8s 200ms/step - loss: 0.3825 - mean_squared_error: 0.8599 - val_loss: 0.3802 - val_mean_squared_error: 0.8413 - lr: 1.0000e-04\n",
      "Epoch 2/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3633 - mean_squared_error: 0.8156\n",
      "Epoch 2: val_loss improved from 0.38015 to 0.36061, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 168ms/step - loss: 0.3633 - mean_squared_error: 0.8156 - val_loss: 0.3606 - val_mean_squared_error: 0.7963 - lr: 1.0593e-04\n",
      "Epoch 3/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3435 - mean_squared_error: 0.7704\n",
      "Epoch 3: val_loss improved from 0.36061 to 0.34045, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 170ms/step - loss: 0.3435 - mean_squared_error: 0.7704 - val_loss: 0.3404 - val_mean_squared_error: 0.7507 - lr: 1.1220e-04\n",
      "Epoch 4/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3229 - mean_squared_error: 0.7249\n",
      "Epoch 4: val_loss improved from 0.34045 to 0.31912, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 168ms/step - loss: 0.3229 - mean_squared_error: 0.7249 - val_loss: 0.3191 - val_mean_squared_error: 0.7036 - lr: 1.1885e-04\n",
      "Epoch 5/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3014 - mean_squared_error: 0.6776\n",
      "Epoch 5: val_loss improved from 0.31912 to 0.29679, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 164ms/step - loss: 0.3014 - mean_squared_error: 0.6776 - val_loss: 0.2968 - val_mean_squared_error: 0.6549 - lr: 1.2589e-04\n",
      "Epoch 6/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.2791 - mean_squared_error: 0.6291\n",
      "Epoch 6: val_loss improved from 0.29679 to 0.27397, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 168ms/step - loss: 0.2791 - mean_squared_error: 0.6291 - val_loss: 0.2740 - val_mean_squared_error: 0.6057 - lr: 1.3335e-04\n",
      "Epoch 7/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.2570 - mean_squared_error: 0.5811\n",
      "Epoch 7: val_loss improved from 0.27397 to 0.25178, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 169ms/step - loss: 0.2570 - mean_squared_error: 0.5811 - val_loss: 0.2518 - val_mean_squared_error: 0.5576 - lr: 1.4125e-04\n",
      "Epoch 8/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.2360 - mean_squared_error: 0.5353\n",
      "Epoch 8: val_loss improved from 0.25178 to 0.23129, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 171ms/step - loss: 0.2360 - mean_squared_error: 0.5353 - val_loss: 0.2313 - val_mean_squared_error: 0.5130 - lr: 1.4962e-04\n",
      "Epoch 9/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.2171 - mean_squared_error: 0.4937\n",
      "Epoch 9: val_loss improved from 0.23129 to 0.21333, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 174ms/step - loss: 0.2171 - mean_squared_error: 0.4937 - val_loss: 0.2133 - val_mean_squared_error: 0.4738 - lr: 1.5849e-04\n",
      "Epoch 10/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.2007 - mean_squared_error: 0.4573\n",
      "Epoch 10: val_loss improved from 0.21333 to 0.19787, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 172ms/step - loss: 0.2007 - mean_squared_error: 0.4573 - val_loss: 0.1979 - val_mean_squared_error: 0.4404 - lr: 1.6788e-04\n",
      "Epoch 11/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1870 - mean_squared_error: 0.4274\n",
      "Epoch 11: val_loss improved from 0.19787 to 0.18493, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 171ms/step - loss: 0.1870 - mean_squared_error: 0.4274 - val_loss: 0.1849 - val_mean_squared_error: 0.4129 - lr: 1.7783e-04\n",
      "Epoch 12/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1759 - mean_squared_error: 0.4034\n",
      "Epoch 12: val_loss improved from 0.18493 to 0.17458, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 170ms/step - loss: 0.1759 - mean_squared_error: 0.4034 - val_loss: 0.1746 - val_mean_squared_error: 0.3914 - lr: 1.8836e-04\n",
      "Epoch 13/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1673 - mean_squared_error: 0.3854\n",
      "Epoch 13: val_loss improved from 0.17458 to 0.16667, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 163ms/step - loss: 0.1673 - mean_squared_error: 0.3854 - val_loss: 0.1667 - val_mean_squared_error: 0.3755 - lr: 1.9953e-04\n",
      "Epoch 14/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1609 - mean_squared_error: 0.3721\n",
      "Epoch 14: val_loss improved from 0.16667 to 0.16099, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 170ms/step - loss: 0.1609 - mean_squared_error: 0.3721 - val_loss: 0.1610 - val_mean_squared_error: 0.3644 - lr: 2.1135e-04\n",
      "Epoch 15/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1567 - mean_squared_error: 0.3640\n",
      "Epoch 15: val_loss improved from 0.16099 to 0.15709, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 166ms/step - loss: 0.1567 - mean_squared_error: 0.3640 - val_loss: 0.1571 - val_mean_squared_error: 0.3574 - lr: 2.2387e-04\n",
      "Epoch 16/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1537 - mean_squared_error: 0.3574\n",
      "Epoch 16: val_loss improved from 0.15709 to 0.15482, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 169ms/step - loss: 0.1537 - mean_squared_error: 0.3574 - val_loss: 0.1548 - val_mean_squared_error: 0.3529 - lr: 2.3714e-04\n",
      "Epoch 17/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1520 - mean_squared_error: 0.3539\n",
      "Epoch 17: val_loss improved from 0.15482 to 0.15337, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 171ms/step - loss: 0.1520 - mean_squared_error: 0.3539 - val_loss: 0.1534 - val_mean_squared_error: 0.3505 - lr: 2.5119e-04\n",
      "Epoch 18/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1510 - mean_squared_error: 0.3532\n",
      "Epoch 18: val_loss improved from 0.15337 to 0.15247, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 172ms/step - loss: 0.1510 - mean_squared_error: 0.3532 - val_loss: 0.1525 - val_mean_squared_error: 0.3495 - lr: 2.6607e-04\n",
      "Epoch 19/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1504 - mean_squared_error: 0.3520\n",
      "Epoch 19: val_loss improved from 0.15247 to 0.15214, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.1504 - mean_squared_error: 0.3520 - val_loss: 0.1521 - val_mean_squared_error: 0.3490 - lr: 2.8184e-04\n",
      "Epoch 20/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1501 - mean_squared_error: 0.3518\n",
      "Epoch 20: val_loss improved from 0.15214 to 0.15186, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 166ms/step - loss: 0.1501 - mean_squared_error: 0.3518 - val_loss: 0.1519 - val_mean_squared_error: 0.3490 - lr: 2.9854e-04\n",
      "Epoch 21/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1500 - mean_squared_error: 0.3521\n",
      "Epoch 21: val_loss improved from 0.15186 to 0.15167, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 170ms/step - loss: 0.1500 - mean_squared_error: 0.3521 - val_loss: 0.1517 - val_mean_squared_error: 0.3492 - lr: 3.1623e-04\n",
      "Epoch 22/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1500 - mean_squared_error: 0.3519\n",
      "Epoch 22: val_loss did not improve from 0.15167\n",
      "23/23 [==============================] - 4s 163ms/step - loss: 0.1500 - mean_squared_error: 0.3519 - val_loss: 0.1517 - val_mean_squared_error: 0.3489 - lr: 3.3497e-04\n",
      "Epoch 23/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1500 - mean_squared_error: 0.3524\n",
      "Epoch 23: val_loss improved from 0.15167 to 0.15157, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 169ms/step - loss: 0.1500 - mean_squared_error: 0.3524 - val_loss: 0.1516 - val_mean_squared_error: 0.3485 - lr: 3.5481e-04\n",
      "Epoch 24/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1499 - mean_squared_error: 0.3520\n",
      "Epoch 24: val_loss improved from 0.15157 to 0.15156, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 165ms/step - loss: 0.1499 - mean_squared_error: 0.3520 - val_loss: 0.1516 - val_mean_squared_error: 0.3486 - lr: 3.7584e-04\n",
      "Epoch 25/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1499 - mean_squared_error: 0.3511\n",
      "Epoch 25: val_loss did not improve from 0.15156\n",
      "23/23 [==============================] - 4s 161ms/step - loss: 0.1499 - mean_squared_error: 0.3511 - val_loss: 0.1516 - val_mean_squared_error: 0.3484 - lr: 3.9811e-04\n",
      "Epoch 26/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1498 - mean_squared_error: 0.3511\n",
      "Epoch 26: val_loss improved from 0.15156 to 0.15146, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 173ms/step - loss: 0.1498 - mean_squared_error: 0.3511 - val_loss: 0.1515 - val_mean_squared_error: 0.3483 - lr: 4.2170e-04\n",
      "Epoch 27/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1498 - mean_squared_error: 0.3524\n",
      "Epoch 27: val_loss improved from 0.15146 to 0.15144, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 179ms/step - loss: 0.1498 - mean_squared_error: 0.3524 - val_loss: 0.1514 - val_mean_squared_error: 0.3485 - lr: 4.4668e-04\n",
      "Epoch 28/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1498 - mean_squared_error: 0.3512\n",
      "Epoch 28: val_loss did not improve from 0.15144\n",
      "23/23 [==============================] - 4s 195ms/step - loss: 0.1498 - mean_squared_error: 0.3512 - val_loss: 0.1514 - val_mean_squared_error: 0.3478 - lr: 4.7315e-04\n",
      "Epoch 29/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1497 - mean_squared_error: 0.3503\n",
      "Epoch 29: val_loss improved from 0.15144 to 0.15126, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 171ms/step - loss: 0.1497 - mean_squared_error: 0.3503 - val_loss: 0.1513 - val_mean_squared_error: 0.3483 - lr: 5.0119e-04\n",
      "Epoch 30/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1497 - mean_squared_error: 0.3523\n",
      "Epoch 30: val_loss improved from 0.15126 to 0.15110, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 170ms/step - loss: 0.1497 - mean_squared_error: 0.3523 - val_loss: 0.1511 - val_mean_squared_error: 0.3477 - lr: 5.3088e-04\n",
      "Epoch 31/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1496 - mean_squared_error: 0.3506\n",
      "Epoch 31: val_loss improved from 0.15110 to 0.15106, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 167ms/step - loss: 0.1496 - mean_squared_error: 0.3506 - val_loss: 0.1511 - val_mean_squared_error: 0.3480 - lr: 5.6234e-04\n",
      "Epoch 32/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1493 - mean_squared_error: 0.3511\n",
      "Epoch 32: val_loss improved from 0.15106 to 0.15083, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 164ms/step - loss: 0.1493 - mean_squared_error: 0.3511 - val_loss: 0.1508 - val_mean_squared_error: 0.3469 - lr: 5.9566e-04\n",
      "Epoch 33/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1494 - mean_squared_error: 0.3493\n",
      "Epoch 33: val_loss improved from 0.15083 to 0.15055, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 168ms/step - loss: 0.1494 - mean_squared_error: 0.3493 - val_loss: 0.1505 - val_mean_squared_error: 0.3473 - lr: 6.3096e-04\n",
      "Epoch 34/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1490 - mean_squared_error: 0.3500\n",
      "Epoch 34: val_loss improved from 0.15055 to 0.15014, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 167ms/step - loss: 0.1490 - mean_squared_error: 0.3500 - val_loss: 0.1501 - val_mean_squared_error: 0.3466 - lr: 6.6834e-04\n",
      "Epoch 35/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1484 - mean_squared_error: 0.3482\n",
      "Epoch 35: val_loss improved from 0.15014 to 0.14950, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 174ms/step - loss: 0.1484 - mean_squared_error: 0.3482 - val_loss: 0.1495 - val_mean_squared_error: 0.3446 - lr: 7.0795e-04\n",
      "Epoch 36/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1475 - mean_squared_error: 0.3457\n",
      "Epoch 36: val_loss improved from 0.14950 to 0.14867, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 171ms/step - loss: 0.1475 - mean_squared_error: 0.3457 - val_loss: 0.1487 - val_mean_squared_error: 0.3427 - lr: 7.4989e-04\n",
      "Epoch 37/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1463 - mean_squared_error: 0.3430\n",
      "Epoch 37: val_loss improved from 0.14867 to 0.14739, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 195ms/step - loss: 0.1463 - mean_squared_error: 0.3430 - val_loss: 0.1474 - val_mean_squared_error: 0.3402 - lr: 7.9433e-04\n",
      "Epoch 38/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1448 - mean_squared_error: 0.3403\n",
      "Epoch 38: val_loss improved from 0.14739 to 0.14519, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 166ms/step - loss: 0.1448 - mean_squared_error: 0.3403 - val_loss: 0.1452 - val_mean_squared_error: 0.3343 - lr: 8.4140e-04\n",
      "Epoch 39/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1415 - mean_squared_error: 0.3311\n",
      "Epoch 39: val_loss improved from 0.14519 to 0.14154, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 169ms/step - loss: 0.1415 - mean_squared_error: 0.3311 - val_loss: 0.1415 - val_mean_squared_error: 0.3264 - lr: 8.9125e-04\n",
      "Epoch 40/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1358 - mean_squared_error: 0.3173\n",
      "Epoch 40: val_loss improved from 0.14154 to 0.13484, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 167ms/step - loss: 0.1358 - mean_squared_error: 0.3173 - val_loss: 0.1348 - val_mean_squared_error: 0.3139 - lr: 9.4406e-04\n",
      "Epoch 41/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1213 - mean_squared_error: 0.2820\n",
      "Epoch 41: val_loss improved from 0.13484 to 0.11953, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 173ms/step - loss: 0.1213 - mean_squared_error: 0.2820 - val_loss: 0.1195 - val_mean_squared_error: 0.2810 - lr: 0.0100\n",
      "Epoch 42/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1109 - mean_squared_error: 0.2511\n",
      "Epoch 42: val_loss did not improve from 0.11953\n",
      "23/23 [==============================] - 4s 165ms/step - loss: 0.1109 - mean_squared_error: 0.2511 - val_loss: 0.1373 - val_mean_squared_error: 0.2942 - lr: 0.0100\n",
      "Epoch 43/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1088 - mean_squared_error: 0.2471\n",
      "Epoch 43: val_loss improved from 0.11953 to 0.11062, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 172ms/step - loss: 0.1088 - mean_squared_error: 0.2471 - val_loss: 0.1106 - val_mean_squared_error: 0.2537 - lr: 0.0100\n",
      "Epoch 44/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0958 - mean_squared_error: 0.2195\n",
      "Epoch 44: val_loss did not improve from 0.11062\n",
      "23/23 [==============================] - 4s 163ms/step - loss: 0.0958 - mean_squared_error: 0.2195 - val_loss: 0.1121 - val_mean_squared_error: 0.2531 - lr: 0.0100\n",
      "Epoch 45/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0953 - mean_squared_error: 0.2163\n",
      "Epoch 45: val_loss improved from 0.11062 to 0.11047, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 166ms/step - loss: 0.0953 - mean_squared_error: 0.2163 - val_loss: 0.1105 - val_mean_squared_error: 0.2502 - lr: 0.0100\n",
      "Epoch 46/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0951 - mean_squared_error: 0.2169\n",
      "Epoch 46: val_loss did not improve from 0.11047\n",
      "23/23 [==============================] - 4s 164ms/step - loss: 0.0951 - mean_squared_error: 0.2169 - val_loss: 0.1109 - val_mean_squared_error: 0.2487 - lr: 0.0100\n",
      "Epoch 47/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0971 - mean_squared_error: 0.2201\n",
      "Epoch 47: val_loss improved from 0.11047 to 0.10842, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 168ms/step - loss: 0.0971 - mean_squared_error: 0.2201 - val_loss: 0.1084 - val_mean_squared_error: 0.2472 - lr: 0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0927 - mean_squared_error: 0.2102\n",
      "Epoch 48: val_loss improved from 0.10842 to 0.10779, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 169ms/step - loss: 0.0927 - mean_squared_error: 0.2102 - val_loss: 0.1078 - val_mean_squared_error: 0.2454 - lr: 0.0100\n",
      "Epoch 49/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0931 - mean_squared_error: 0.2123\n",
      "Epoch 49: val_loss did not improve from 0.10779\n",
      "23/23 [==============================] - 4s 160ms/step - loss: 0.0931 - mean_squared_error: 0.2123 - val_loss: 0.1130 - val_mean_squared_error: 0.2519 - lr: 0.0100\n",
      "Epoch 50/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0927 - mean_squared_error: 0.2104\n",
      "Epoch 50: val_loss improved from 0.10779 to 0.10680, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 170ms/step - loss: 0.0927 - mean_squared_error: 0.2104 - val_loss: 0.1068 - val_mean_squared_error: 0.2448 - lr: 0.0100\n",
      "Epoch 51/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0919 - mean_squared_error: 0.2090\n",
      "Epoch 51: val_loss improved from 0.10680 to 0.10612, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 166ms/step - loss: 0.0919 - mean_squared_error: 0.2090 - val_loss: 0.1061 - val_mean_squared_error: 0.2461 - lr: 0.0100\n",
      "Epoch 52/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0920 - mean_squared_error: 0.2103\n",
      "Epoch 52: val_loss did not improve from 0.10612\n",
      "23/23 [==============================] - 4s 161ms/step - loss: 0.0920 - mean_squared_error: 0.2103 - val_loss: 0.1079 - val_mean_squared_error: 0.2423 - lr: 0.0100\n",
      "Epoch 53/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0924 - mean_squared_error: 0.2096\n",
      "Epoch 53: val_loss did not improve from 0.10612\n",
      "23/23 [==============================] - 4s 160ms/step - loss: 0.0924 - mean_squared_error: 0.2096 - val_loss: 0.1108 - val_mean_squared_error: 0.2595 - lr: 0.0100\n",
      "Epoch 54/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0936 - mean_squared_error: 0.2130\n",
      "Epoch 54: val_loss did not improve from 0.10612\n",
      "23/23 [==============================] - 4s 163ms/step - loss: 0.0936 - mean_squared_error: 0.2130 - val_loss: 0.1173 - val_mean_squared_error: 0.2774 - lr: 0.0100\n",
      "Epoch 55/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0957 - mean_squared_error: 0.2201\n",
      "Epoch 55: val_loss did not improve from 0.10612\n",
      "23/23 [==============================] - 4s 163ms/step - loss: 0.0957 - mean_squared_error: 0.2201 - val_loss: 0.1082 - val_mean_squared_error: 0.2501 - lr: 0.0100\n",
      "Epoch 56/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0938 - mean_squared_error: 0.2152\n",
      "Epoch 56: val_loss did not improve from 0.10612\n",
      "23/23 [==============================] - 4s 163ms/step - loss: 0.0938 - mean_squared_error: 0.2152 - val_loss: 0.1072 - val_mean_squared_error: 0.2451 - lr: 0.0100\n",
      "Epoch 57/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0906 - mean_squared_error: 0.2056\n",
      "Epoch 57: val_loss improved from 0.10612 to 0.10561, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 169ms/step - loss: 0.0906 - mean_squared_error: 0.2056 - val_loss: 0.1056 - val_mean_squared_error: 0.2449 - lr: 0.0100\n",
      "Epoch 58/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0912 - mean_squared_error: 0.2072\n",
      "Epoch 58: val_loss did not improve from 0.10561\n",
      "23/23 [==============================] - 4s 162ms/step - loss: 0.0912 - mean_squared_error: 0.2072 - val_loss: 0.1073 - val_mean_squared_error: 0.2516 - lr: 0.0100\n",
      "Epoch 59/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0921 - mean_squared_error: 0.2124\n",
      "Epoch 59: val_loss did not improve from 0.10561\n",
      "23/23 [==============================] - 4s 163ms/step - loss: 0.0921 - mean_squared_error: 0.2124 - val_loss: 0.1062 - val_mean_squared_error: 0.2442 - lr: 0.0100\n",
      "Epoch 60/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0906 - mean_squared_error: 0.2078\n",
      "Epoch 60: val_loss did not improve from 0.10561\n",
      "23/23 [==============================] - 4s 162ms/step - loss: 0.0906 - mean_squared_error: 0.2078 - val_loss: 0.1076 - val_mean_squared_error: 0.2456 - lr: 0.0100\n",
      "Epoch 61/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0949 - mean_squared_error: 0.2154\n",
      "Epoch 61: val_loss did not improve from 0.10561\n",
      "23/23 [==============================] - 4s 163ms/step - loss: 0.0949 - mean_squared_error: 0.2154 - val_loss: 0.1077 - val_mean_squared_error: 0.2435 - lr: 0.0100\n",
      "Epoch 62/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0908 - mean_squared_error: 0.2087\n",
      "Epoch 62: val_loss did not improve from 0.10561\n",
      "23/23 [==============================] - 4s 165ms/step - loss: 0.0908 - mean_squared_error: 0.2087 - val_loss: 0.1064 - val_mean_squared_error: 0.2468 - lr: 0.0100\n",
      "Epoch 63/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0901 - mean_squared_error: 0.2059\n",
      "Epoch 63: val_loss did not improve from 0.10561\n",
      "23/23 [==============================] - 4s 163ms/step - loss: 0.0901 - mean_squared_error: 0.2059 - val_loss: 0.1088 - val_mean_squared_error: 0.2432 - lr: 0.0100\n",
      "Epoch 64/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0913 - mean_squared_error: 0.2075\n",
      "Epoch 64: val_loss did not improve from 0.10561\n",
      "23/23 [==============================] - 4s 164ms/step - loss: 0.0913 - mean_squared_error: 0.2075 - val_loss: 0.1077 - val_mean_squared_error: 0.2437 - lr: 0.0100\n",
      "Epoch 65/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0902 - mean_squared_error: 0.2068\n",
      "Epoch 65: val_loss did not improve from 0.10561\n",
      "23/23 [==============================] - 4s 162ms/step - loss: 0.0902 - mean_squared_error: 0.2068 - val_loss: 0.1066 - val_mean_squared_error: 0.2407 - lr: 0.0100\n",
      "Epoch 66/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0904 - mean_squared_error: 0.2057\n",
      "Epoch 66: val_loss did not improve from 0.10561\n",
      "23/23 [==============================] - 4s 162ms/step - loss: 0.0904 - mean_squared_error: 0.2057 - val_loss: 0.1068 - val_mean_squared_error: 0.2417 - lr: 0.0100\n",
      "Epoch 67/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0908 - mean_squared_error: 0.2078\n",
      "Epoch 67: val_loss did not improve from 0.10561\n",
      "23/23 [==============================] - 4s 165ms/step - loss: 0.0908 - mean_squared_error: 0.2078 - val_loss: 0.1078 - val_mean_squared_error: 0.2433 - lr: 0.0100\n",
      "Epoch 68/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0891 - mean_squared_error: 0.2022\n",
      "Epoch 68: val_loss did not improve from 0.10561\n",
      "23/23 [==============================] - 4s 163ms/step - loss: 0.0891 - mean_squared_error: 0.2022 - val_loss: 0.1120 - val_mean_squared_error: 0.2655 - lr: 0.0100\n",
      "Epoch 69/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0936 - mean_squared_error: 0.2123\n",
      "Epoch 69: val_loss did not improve from 0.10561\n",
      "23/23 [==============================] - 4s 162ms/step - loss: 0.0936 - mean_squared_error: 0.2123 - val_loss: 0.1118 - val_mean_squared_error: 0.2641 - lr: 0.0100\n",
      "Epoch 70/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0927 - mean_squared_error: 0.2140\n",
      "Epoch 70: val_loss did not improve from 0.10561\n",
      "23/23 [==============================] - 4s 161ms/step - loss: 0.0927 - mean_squared_error: 0.2140 - val_loss: 0.1096 - val_mean_squared_error: 0.2466 - lr: 0.0100\n",
      "Epoch 71/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0900 - mean_squared_error: 0.2059\n",
      "Epoch 71: val_loss did not improve from 0.10561\n",
      "23/23 [==============================] - 4s 161ms/step - loss: 0.0900 - mean_squared_error: 0.2059 - val_loss: 0.1079 - val_mean_squared_error: 0.2459 - lr: 0.0100\n",
      "Epoch 72/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0930 - mean_squared_error: 0.2125\n",
      "Epoch 72: val_loss improved from 0.10561 to 0.10527, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 168ms/step - loss: 0.0930 - mean_squared_error: 0.2125 - val_loss: 0.1053 - val_mean_squared_error: 0.2437 - lr: 0.0100\n",
      "Epoch 73/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0898 - mean_squared_error: 0.2059\n",
      "Epoch 73: val_loss improved from 0.10527 to 0.10505, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 167ms/step - loss: 0.0898 - mean_squared_error: 0.2059 - val_loss: 0.1050 - val_mean_squared_error: 0.2404 - lr: 0.0100\n",
      "Epoch 74/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0895 - mean_squared_error: 0.2039\n",
      "Epoch 74: val_loss did not improve from 0.10505\n",
      "23/23 [==============================] - 4s 163ms/step - loss: 0.0895 - mean_squared_error: 0.2039 - val_loss: 0.1061 - val_mean_squared_error: 0.2487 - lr: 0.0100\n",
      "Epoch 75/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0895 - mean_squared_error: 0.2049\n",
      "Epoch 75: val_loss did not improve from 0.10505\n",
      "23/23 [==============================] - 4s 160ms/step - loss: 0.0895 - mean_squared_error: 0.2049 - val_loss: 0.1107 - val_mean_squared_error: 0.2482 - lr: 0.0100\n",
      "Epoch 76/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0889 - mean_squared_error: 0.2040\n",
      "Epoch 76: val_loss did not improve from 0.10505\n",
      "23/23 [==============================] - 4s 165ms/step - loss: 0.0889 - mean_squared_error: 0.2040 - val_loss: 0.1078 - val_mean_squared_error: 0.2435 - lr: 0.0100\n",
      "Epoch 77/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0895 - mean_squared_error: 0.2040\n",
      "Epoch 77: val_loss did not improve from 0.10505\n",
      "23/23 [==============================] - 4s 166ms/step - loss: 0.0895 - mean_squared_error: 0.2040 - val_loss: 0.1102 - val_mean_squared_error: 0.2451 - lr: 0.0100\n",
      "Epoch 78/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0914 - mean_squared_error: 0.2089\n",
      "Epoch 78: val_loss did not improve from 0.10505\n",
      "23/23 [==============================] - 4s 163ms/step - loss: 0.0914 - mean_squared_error: 0.2089 - val_loss: 0.1053 - val_mean_squared_error: 0.2389 - lr: 0.0100\n",
      "Epoch 79/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0905 - mean_squared_error: 0.2065\n",
      "Epoch 79: val_loss did not improve from 0.10505\n",
      "23/23 [==============================] - 4s 163ms/step - loss: 0.0905 - mean_squared_error: 0.2065 - val_loss: 0.1110 - val_mean_squared_error: 0.2597 - lr: 0.0100\n",
      "Epoch 80/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0917 - mean_squared_error: 0.2084\n",
      "Epoch 80: val_loss did not improve from 0.10505\n",
      "23/23 [==============================] - 4s 174ms/step - loss: 0.0917 - mean_squared_error: 0.2084 - val_loss: 0.1161 - val_mean_squared_error: 0.2763 - lr: 0.0100\n",
      "Epoch 81/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0904 - mean_squared_error: 0.2070\n",
      "Epoch 81: val_loss did not improve from 0.10505\n",
      "23/23 [==============================] - 4s 170ms/step - loss: 0.0904 - mean_squared_error: 0.2070 - val_loss: 0.1060 - val_mean_squared_error: 0.2422 - lr: 0.0100\n",
      "Epoch 82/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0897 - mean_squared_error: 0.2048\n",
      "Epoch 82: val_loss did not improve from 0.10505\n",
      "23/23 [==============================] - 4s 163ms/step - loss: 0.0897 - mean_squared_error: 0.2048 - val_loss: 0.1143 - val_mean_squared_error: 0.2725 - lr: 0.0100\n",
      "Epoch 83/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0889 - mean_squared_error: 0.2038\n",
      "Epoch 83: val_loss did not improve from 0.10505\n",
      "23/23 [==============================] - 4s 161ms/step - loss: 0.0889 - mean_squared_error: 0.2038 - val_loss: 0.1057 - val_mean_squared_error: 0.2415 - lr: 0.0100\n",
      "Epoch 84/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0894 - mean_squared_error: 0.2039\n",
      "Epoch 84: val_loss did not improve from 0.10505\n",
      "23/23 [==============================] - 4s 162ms/step - loss: 0.0894 - mean_squared_error: 0.2039 - val_loss: 0.1085 - val_mean_squared_error: 0.2567 - lr: 0.0100\n",
      "Epoch 85/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0890 - mean_squared_error: 0.2037\n",
      "Epoch 85: val_loss did not improve from 0.10505\n",
      "23/23 [==============================] - 4s 168ms/step - loss: 0.0890 - mean_squared_error: 0.2037 - val_loss: 0.1060 - val_mean_squared_error: 0.2436 - lr: 0.0100\n",
      "Epoch 86/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0881 - mean_squared_error: 0.2014\n",
      "Epoch 86: val_loss did not improve from 0.10505\n",
      "23/23 [==============================] - 4s 170ms/step - loss: 0.0881 - mean_squared_error: 0.2014 - val_loss: 0.1067 - val_mean_squared_error: 0.2468 - lr: 0.0100\n",
      "Epoch 87/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0883 - mean_squared_error: 0.2010\n",
      "Epoch 87: val_loss did not improve from 0.10505\n",
      "23/23 [==============================] - 4s 165ms/step - loss: 0.0883 - mean_squared_error: 0.2010 - val_loss: 0.1066 - val_mean_squared_error: 0.2473 - lr: 0.0100\n",
      "Epoch 88/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0887 - mean_squared_error: 0.2047\n",
      "Epoch 88: val_loss did not improve from 0.10505\n",
      "23/23 [==============================] - 4s 163ms/step - loss: 0.0887 - mean_squared_error: 0.2047 - val_loss: 0.1074 - val_mean_squared_error: 0.2443 - lr: 0.0100\n",
      "Epoch 89/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0890 - mean_squared_error: 0.2018\n",
      "Epoch 89: val_loss improved from 0.10505 to 0.10466, saving model to best_model.h5\n",
      "23/23 [==============================] - 4s 169ms/step - loss: 0.0890 - mean_squared_error: 0.2018 - val_loss: 0.1047 - val_mean_squared_error: 0.2412 - lr: 0.0100\n",
      "Epoch 90/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0896 - mean_squared_error: 0.2067\n",
      "Epoch 90: val_loss did not improve from 0.10466\n",
      "23/23 [==============================] - 4s 163ms/step - loss: 0.0896 - mean_squared_error: 0.2067 - val_loss: 0.1132 - val_mean_squared_error: 0.2501 - lr: 0.0100\n",
      "Epoch 91/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0893 - mean_squared_error: 0.2033\n",
      "Epoch 91: val_loss did not improve from 0.10466\n",
      "23/23 [==============================] - 4s 162ms/step - loss: 0.0893 - mean_squared_error: 0.2033 - val_loss: 0.1110 - val_mean_squared_error: 0.2579 - lr: 0.0100\n",
      "Epoch 92/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0902 - mean_squared_error: 0.2065\n",
      "Epoch 92: val_loss did not improve from 0.10466\n",
      "23/23 [==============================] - 4s 163ms/step - loss: 0.0902 - mean_squared_error: 0.2065 - val_loss: 0.1049 - val_mean_squared_error: 0.2441 - lr: 0.0100\n",
      "Epoch 93/200\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0889 - mean_squared_error: 0.2044\n",
      "Epoch 93: val_loss did not improve from 0.10466\n",
      "23/23 [==============================] - 4s 163ms/step - loss: 0.0889 - mean_squared_error: 0.2044 - val_loss: 0.1055 - val_mean_squared_error: 0.2383 - lr: 0.0100\n",
      "Epoch 93: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=train_data,y=train_lab, epochs=MAX_EPOCH, batch_size=BATCH_SIZE,validation_data=(val_data, val_lab), verbose=1,callbacks=[lr_scheduler, model_chk,ear_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6cb26518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 44ms/step - loss: 0.0804 - mean_squared_error: 0.1835\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08036354184150696, 0.1835242509841919]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x=test_data,y=test_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3245c77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de06ebc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
